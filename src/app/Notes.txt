1. Bug: 
    - a personalização de ids PRECISA ser arrumada em createStructure(); (testar em movielens e yahoo, precisa arrumar uns detalhes)
    - legenda dos factors não está sendo escrita no arquivo -> pode ser o problema das barras linux e windows
    - Ver se essas funções ficam no framework (removeBottom, removeBottomUsers, removebelowID)
    - Yahoo: id 14994 teste se tem similaridade negativa
    - warmup não está totalmente automatizado (depois de acrescentar oneFacMultiLvlExp)
    - GERADOR DE GRÁFICOS NO WINDOWS NÃO DESENHA DIREITO; TESTAR COM ID 149
    - multi_lvl_starter de evaluation tem que ser salvo na função saveEvaluation quando for exp multi lvl
    - Fazer o line chart salvar em arquivo igual o de barras
    - criar um Utils.computePopularity();
    - (notado no RSE_DDMX): a lista de recomendação não está sendo ordenada corretamente em alguns casos
    - (notado no RSE_DDMX): no multilevelexp, aparentemente os valores dos fatores nmo gráfico dos resulados estão deslocados de uma unidade
    - Algumas tabelas estão chamando item_similarity e outras item_similarity_new (o mesmo para user e vectors). Atualmente a oficial é a new. Isso tá acontecendo tanto no bd quanto no software. 
    - Reverter duas condições no RSE: no benchmarker → pilot2, resp.awake (exp e multilvlexp) e desvio padrão?? (conferir o desvio padrão calculado no pilot2 e aquele salvo na evaluation)
    - Menu no app.java roteando para as diferentes mains
    - ver se algo mais pode ser melhorado, documentar, dar uma olhada geral no código
    - o código da DDMX pode ser enxugado, ter os nomes das constantes alterados, etc...
    - arrumar o index de int filter[] = {0,4} = exp 1 e 5
    - db pass deixar por padrão ""
    - ver na versão do pendrive se tem user user r2 17 nov em uma das mains

2.
    . Para o experimento realizado no artigo, conferir a ordem dos fatores da legenda, que está diferente da ordem realizada no experimento, mas aparentemente a legenda parece correta.
    . Para os demais, salvar no banco que acaba com as dúvidas na legenda:
    
    Fator: T_CANDIDATES_SIZE
        Fator Composto null

    Fator: T_NEIGHBOORHOOD_SIZE
        Fator Composto null

    Fator: T_RECOMMENDATION_LIST_LENGTH
        Fator Composto null

    A: T_CANDIDATES_SIZE
    B: T_NEIGHBOORHOOD_SIZE
    C: T_RECOMMENDATION_LIST_LENGTH


    no exp: 
    Evaluation id: 85
    Fator: T_NEIGHBOORHOOD_SIZE
        Fator Composto null

    Fator: T_CANDIDATES_SIZE
        Fator Composto null

    Fator: T_RECOMMENDATION_LIST_LENGTH
        Fator Composto null


===============================================================================================
**************************** Outros menos importantes *****************************************
===============================================================================================

1. BUG: quando o fator composto possui mais de dois niveis, o software dá pau em Benchmarker -> saveFactor
. Em factorInfluence: pieChart tem que ser chamado depois de fi.factorsWeight();
. Se só fi.factorsWeight(); for chamado em um loop dá pau nos fatores. Entender pq isso ta acontecendo

2. Variáveis de Resposta que podem ser implementadas
    - Se for implementar curvas precision-recall ou ROC, ver  Materiais -> [6; Curvas Precision-Recall e ROC]
    - ROC (explicação em Evaluating Collaborative Filtering Recommender Systems); CROC (pelo que entendi, variação da ROC usando P@N) ;;; ROC curve para cada user and average the resulting curves over users. 
    - Porcentagem da lista que é formada por recomendação alternativa
    - Erro médio por user ou Distribuição de erros por users (accuracy and error->W1->4_Prediction Accuracy Metrics: 6min)
    - Breese Score (Decision and Suport)
    - Ad Hoc (Decision and Suport)
    - Mean Reciprocal Rank (Rank Aware)
    - Mean Average Precision (Rank Aware)
    - Personalization (User-Centered)
    - Serendipity (User-Centered)
    - Diversity (ILS é uma forma) (User-Centered)


3. Multiple Pie Charts (com todas as variáveis de respostas)
        http://www.java2s.com/Code/Java/Chart/JFreeChartMultiplePieChartDemo1.htm
        http://www.java2s.com/Code/Java/Chart/JFreeChartMultiplePieChartDemo4.htm

4. Uma boa ideia para a conferência das variáveis de resposta é salvar em bd a recommendation table e o que mais for preciso 

5. Melhorias da performance SQL:
    - PreparedStatement, st.executeBatch e executeLargeBatch, PostgreSQL VACUUM
    - desempenho do "in" vs "join
    - bulk insert e bulk update

6. Embora o recomendador seja de livre implementação, o RSE padroniza a forma de mostrar a lista, utilizando a maneira 
   popular de ranqueamento, que é ordená-la em ordem decrescente das predições. Entretanto, o trabalho de [6;1] sugere que 
   filmes com as avaliações mais altas nem sempre são os mais assistidos. Isso leva a criação de outras formas de ranqueamento,
   que podem inclusive ser usadas como fator e comparadas entre si.

7. Gráfico de kiviat para apresentação dos resultados [ver se compensa no livro de estatística]

8. Escolha do indice de tendencia central e indice de dispersão; pode ser interessante colocar outras opçoes desses tipos.
    -> Error bars often represent one standard deviation of uncertainty, one standard error, or a particular confidence interval
     (e.g., a 95\% interval). These quantities are not the same and so the measure selected should be stated explicitly in 
     the graph or supporting text. Texto interessante sobre: http://berkeleysciencereview.com/errorbars-anyway/

9. Os usuários são divididos em 5 partes aleatorias de tamanhos iguais. Em cada experiemento, a carga de trabalho é um 
    subconjunto da parte ativa. Entretanto são pegos sempre os primeiros usuários. Então, por exemplo, as pessoas da 
    repetição 3 são as mesmas da 8. Seria bom que essa parte também fosse randomizada.

10. Criar módulo para ver a recomendação como um sistema normal.

11. Possibilitar a geração de relatórios
